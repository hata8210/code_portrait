{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/5q/126l7pmx5072v33dml8wq3640000gn/T/jieba.cache\n",
      "Loading model cost 2.644 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对 p\n",
      "这 r\n",
      "句 q\n",
      "话 n\n",
      "进行 v\n",
      "分词 n\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg  \n",
    "words=pseg.cut(\"对这句话进行分词\")  \n",
    "for key in words:  \n",
    "     print(key.word,key.flag)\n",
    "\n",
    "# 结巴分词的官方github\n",
    "# https://github.com/fxsjy/jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "他 来到 了 网易 杭研 大厦\n",
      "-------这里输出第 0 类文本的词语tf-idf权重------\n",
      "中国 0.0\n",
      "北京 0.52640543361\n",
      "大厦 0.0\n",
      "天安门 0.0\n",
      "小明 0.0\n",
      "来到 0.52640543361\n",
      "杭研 0.0\n",
      "毕业 0.0\n",
      "清华大学 0.66767854461\n",
      "硕士 0.0\n",
      "科学院 0.0\n",
      "网易 0.0\n",
      "-------这里输出第 1 类文本的词语tf-idf权重------\n",
      "中国 0.0\n",
      "北京 0.0\n",
      "大厦 0.525472749264\n",
      "天安门 0.0\n",
      "小明 0.0\n",
      "来到 0.414288751166\n",
      "杭研 0.525472749264\n",
      "毕业 0.0\n",
      "清华大学 0.0\n",
      "硕士 0.0\n",
      "科学院 0.0\n",
      "网易 0.525472749264\n",
      "-------这里输出第 2 类文本的词语tf-idf权重------\n",
      "中国 0.4472135955\n",
      "北京 0.0\n",
      "大厦 0.0\n",
      "天安门 0.0\n",
      "小明 0.4472135955\n",
      "来到 0.0\n",
      "杭研 0.0\n",
      "毕业 0.4472135955\n",
      "清华大学 0.0\n",
      "硕士 0.4472135955\n",
      "科学院 0.4472135955\n",
      "网易 0.0\n",
      "-------这里输出第 3 类文本的词语tf-idf权重------\n",
      "中国 0.0\n",
      "北京 0.61913029649\n",
      "大厦 0.0\n",
      "天安门 0.78528827571\n",
      "小明 0.0\n",
      "来到 0.0\n",
      "杭研 0.0\n",
      "毕业 0.0\n",
      "清华大学 0.0\n",
      "硕士 0.0\n",
      "科学院 0.0\n",
      "网易 0.0\n"
     ]
    }
   ],
   "source": [
    "#pip install jieba\n",
    "__author__ = \"hata\"  \n",
    "import jieba  \n",
    "import jieba.posseg as pseg  \n",
    "import os  \n",
    "import sys  \n",
    "from sklearn import feature_extraction  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "  \n",
    "if __name__ == \"__main__\":  \n",
    "    corpus=[\"我 来到 北京 清华大学\",#第一类文本切词后的结果，词之间以空格隔开  \n",
    "        \"他 来到 了 网易 杭研 大厦\",#第二类文本的切词结果  \n",
    "        \"小明 硕士 毕业 与 中国 科学院\",#第三类文本的切词结果  \n",
    "        \"我 爱 北京 天安门\"]#第四类文本的切词结果  \n",
    "    print(corpus[1])\n",
    "    vectorizer=CountVectorizer()#该类会将文本中的词语转换为词频矩阵，矩阵元素a[i][j] 表示j词在i类文本下的词频  \n",
    "    transformer=TfidfTransformer()#该类会统计每个词语的tf-idf权值  \n",
    "    tfidf=transformer.fit_transform(vectorizer.fit_transform(corpus))#第一个fit_transform是计算tf-idf，第二个fit_transform是将文本转为词频矩阵  \n",
    "    word=vectorizer.get_feature_names()#获取词袋模型中的所有词语  \n",
    "    weight=tfidf.toarray()#将tf-idf矩阵抽取出来，元素a[i][j]表示j词在i类文本中的tf-idf权重  \n",
    "    for i in range(len(weight)):#打印每类文本的tf-idf词语权重，第一个for遍历所有文本，第二个for便利某一类文本下的词语权重  \n",
    "        print(u\"-------这里输出第\",i,u\"类文本的词语tf-idf权重------\")\n",
    "        for j in range(len(word)):  \n",
    "            print(word[j],weight[i][j])\n",
    "\n",
    "# 关于CountVectorizer和TfidfTransformer介绍\n",
    "# http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
